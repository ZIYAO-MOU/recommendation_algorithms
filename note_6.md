# GBDT+LR

该模型利用GBDT自动进行特征筛选和组合， 进而生成新的离散特征向量， 再把该特征向量当做LR模型的输入， 来产生最后的预测结果。

### LR（逻辑回归）

**逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的**

1. 将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转成**数值型向量**
2. 确定逻辑回归的优化目标，比如把点击率预测转换成**二分类问题**， 这样就可以得到分类问题常用的损失作为目标， 训练模型
3. 在预测的时候， 将**特征向量输入模型产生预测**， 得到用户“点击”物品的概率
4. 利用**点击概率对候选物品排序**， 得到推荐列表

$$
J(w)=-\frac{1}{m}\left(\sum_{i=1}^{m}\left(y^{i} \log f_{w}\left(x^{i}\right)+\left(1-y^{i}\right) \log \left(1-f_{w}\left(x^{i}\right)\right)\right)\right.
$$

求导之后的方式长这样：
$$
w_{j} \leftarrow w_{j}-\gamma \frac{1}{m} \sum_{i=1}^{m}\left(f_{w}\left(x^{i}\right)-y^{i}\right) x_{j}^{i}
$$

#### 优点

1. 特征进行线性加权，所以**性能比较好**，往往适合处理**海量id类特征**，用id类特征有一个很重要的好处，就是**防止信息损失**（相对于范化的 CTR 特征），对于头部资源会有更细致的描述。
2. 只需要存储权重比较大的特征及特征对应的权重。
3. 资源占用小,尤其是内存。在实际的工程应用中只需要存储权重比较大的特征及特征对应的权重。
4. 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)

#### 局限性

1. 无法特征交叉， 特征筛选
2. 准确率并不是很高。因为这毕竟是一个线性模型加了个sigmoid， 形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布
3. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据， 如果想处理非线性， 首先对连续特征的处理需要先进行**离散化**（离散化的目的是为了引入非线性），如上文所说，人工分桶的方式会引入多种问题。
4. LR 需要进行**人工特征组合**

**自动发现有效的特征、特征组合，弥补人工经验不足，缩短LR特征实验周期**

-------》可以通过GBDT

### GBDT

**gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的**， 而这里的残差指的就是当前模型的负梯度值， 这个就要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的， 而**gbdt 无论用于分类还是回归一直都是使用的CART 回归树**

损失函数和LR一样
$$
J(w)=-\frac{1}{m}\left(\sum_{i=1}^{m}\left(y^{i} \log f_{w}\left(x^{i}\right)+\left(1-y^{i}\right) \log \left(1-f_{w}\left(x^{i}\right)\right)\right)\right.
$$
 GBDT二分类也是如此， 用一系列的梯度提升树去拟合这个对数几率， 其分类模型可以表达为：
$$
P(Y=1 \mid x)=\frac{1}{1+e^{-F_{M}(x)}}
$$

1. 初始化GBDT
   和回归问题一样， 分类 GBDT 的初始状态也只有一个叶子节点，该节点为所有样本的初始预测值，如下：

$$
F_{0}(x)=\arg \min _{\gamma} \sum_{i=1}^{n} L(y, \gamma)
$$

2. 循环生成决策树

   - 计算负梯度得到残差
     $$
     r_{i m}=-\left[\frac{\partial L\left(y_{i}, F\left(x_{i}\right)\right)}{\partial F\left(x_{i}\right)}\right]_{F(x)=F_{m-1}(x)}
     $$

   - 使用回归树来拟合 rim

   - 对于每个叶子节点 j , 计算最佳残差拟合值
     $$
     γjm=argminγ∑x∈RijL(yi,Fm−1(xi)+γ)
     $$
     **使用二阶泰勒公式来近似表示该式**，里就相当于把
     $$
     L(y1,Fm−1(x1))
     $$
      当做常量 f(x) ， γ 作为变量 Δx ， 将 f(x) 二阶展开：

$$
L(y1,Fm−1(x1)+γ)≈L(y1,Fm−1(x1))+L′(y1,Fm−1(x1))γ+12L′′(y1,Fm−1(x1))γ2
$$

​				对其求导后得到γ

​				对于任意叶子节点， 我们可以直接计算其输出值：
$$
γjm=∑Riji=1rim∑Riji=1pi,m−1(1−pi,m−1)
$$

#### 优缺点

另外，对于**连续型特征**的处理，GBDT 可以拆分出一个临界阈值，这样就非常轻松的解决了逻辑回归那里**自动发现特征并进行有效组合**的问题， 这也是GBDT的优势所在。

但是GBDT也会有一些局限性， 对于**海量的 id 类特征**，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，当 GBDT 的 one hot 特征大于 10 万维时，**就必须做分布式的训练才能保证不爆内存**。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有**信息损失**，对于头部资源不能有效的表达。

